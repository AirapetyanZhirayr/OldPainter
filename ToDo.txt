	 * add camera preprocessing to renderer [x]
 * add root media dir [x]
 * add fictive white to centroids. Do not use it as a color in palette. Use it to identify white from camera image.
 * count up numbers of different colors during batch
 * change init distribution for strokes width and height from uniform to normal


======== ADDONS
 * what if v_canvas and reference are identical? 
(This situation is possible when drawing on patch of one distinct color). The best solution is not to draw strokes at all in that region.

1.

Сколько видов картинок при работе с камерой бы будем сохранять?

Камера с картинки + ее обработанная версия + виртуальный канвас + анимация

===============================
Проблемы:
	1. Отображение из цветов робота в цвета рисовальщика
	2. Равномерность рисования.

Нужна эвристика, позволяющая понять насколько нужен мазок в данном патче. Если ошибка маленькая или сумма ошибок меньше некоторой epsilon, то стоит передать текущий мазок в следующий патч. 

Как формируются параметры? Как матрица вида [m_grid, m_grid, n_strokes, 9]. То что ты говоришь означало бы разбиение всей этой структуры. Что мне совсем не нравится. Причем фича с добавлением числа рисуемых строк на одном патче в зависимости от разбиения эту структуру не бьет. Нужна просто догадка, функция связывающая разбиения с числом наносимых мазков.

Логично было бы уравновесить число наносимых мазков за батч?
Звучит здорово. Тогда у нас будет допустим три батча, три кисти.  

Есть m мазков.




device: cpu
FAST TESTING IS ENABLED
TOTAL STROKES : 216
STROKES PER BLOCK : 4
initialize network with normal
OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'
OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'
loading renderer from pre-trained checkpoint...
torch.Size([4, 3, 128, 128])
torch.Size([4, 3, 128, 128])
iteration step 0, G_loss: 0.61251, step_acc: 3.19922, grid_scale: 2 / 5, strokes: 1 / 4
iteration step 1, G_loss: 0.63024, step_acc: 3.05603, grid_scale: 2 / 5, strokes: 1 / 4
torch.Size([4, 3, 128, 128])
torch.Size([4, 3, 128, 128])
iteration step 2, G_loss: 0.60128, step_acc: 3.28696, grid_scale: 2 / 5, strokes: 2 / 4
iteration step 3, G_loss: 0.61978, step_acc: 3.14720, grid_scale: 2 / 5, strokes: 2 / 4
torch.Size([4, 3, 128, 128])
torch.Size([4, 3, 128, 128])
iteration step 4, G_loss: 0.58334, step_acc: 3.43848, grid_scale: 2 / 5, strokes: 3 / 4
iteration step 5, G_loss: 0.61250, step_acc: 3.20843, grid_scale: 2 / 5, strokes: 3 / 4
torch.Size([4, 3, 128, 128])
torch.Size([4, 3, 128, 128])
iteration step 6, G_loss: 0.58190, step_acc: 3.47964, grid_scale: 2 / 5, strokes: 4 / 4
iteration step 7, G_loss: 0.60185, step_acc: 3.30172, grid_scale: 2 / 5, strokes: 4 / 4
rendering canvas...
saving final rendered result...
READING batch_0_out.pkl
torch.Size([9, 3, 128, 128])
torch.Size([9, 3, 128, 128])
torch.Size([9, 3, 128, 128])
iteration step 0, G_loss: 0.29693, step_acc: 7.06581, grid_scale: 3 / 5, strokes: 1 / 4
iteration step 1, G_loss: 0.29733, step_acc: 7.05504, grid_scale: 3 / 5, strokes: 1 / 4
torch.Size([9, 3, 128, 128])
torch.Size([9, 3, 128, 128])
iteration step 2, G_loss: 0.28077, step_acc: 7.39903, grid_scale: 3 / 5, strokes: 2 / 4
iteration step 3, G_loss: 0.27944, step_acc: 7.42966, grid_scale: 3 / 5, strokes: 2 / 4
torch.Size([9, 3, 128, 128])
torch.Size([9, 3, 128, 128])
iteration step 4, G_loss: 0.25942, step_acc: 7.84972, grid_scale: 3 / 5, strokes: 3 / 4
iteration step 5, G_loss: 0.26595, step_acc: 7.71924, grid_scale: 3 / 5, strokes: 3 / 4
torch.Size([9, 3, 128, 128])
torch.Size([9, 3, 128, 128])
iteration step 6, G_loss: 0.24891, step_acc: 8.12793, grid_scale: 3 / 5, strokes: 4 / 4
iteration step 7, G_loss: 0.24939, step_acc: 8.10461, grid_scale: 3 / 5, strokes: 4 / 4
rendering canvas...
saving final rendered result...
READING batch_1_out.pkl
torch.Size([16, 3, 128, 128])
torch.Size([16, 3, 128, 128])
torch.Size([16, 3, 128, 128])
iteration step 0, G_loss: 0.15139, step_acc: 11.33762, grid_scale: 4 / 5, strokes: 1 / 4
iteration step 1, G_loss: 0.15117, step_acc: 11.33828, grid_scale: 4 / 5, strokes: 1 / 4
torch.Size([16, 3, 128, 128])
torch.Size([16, 3, 128, 128])
iteration step 2, G_loss: 0.13737, step_acc: 12.03047, grid_scale: 4 / 5, strokes: 2 / 4
iteration step 3, G_loss: 0.13648, step_acc: 12.05581, grid_scale: 4 / 5, strokes: 2 / 4
torch.Size([16, 3, 128, 128])
torch.Size([16, 3, 128, 128])
iteration step 4, G_loss: 0.12602, step_acc: 12.61652, grid_scale: 4 / 5, strokes: 3 / 4
iteration step 5, G_loss: 0.12373, step_acc: 12.74806, grid_scale: 4 / 5, strokes: 3 / 4
torch.Size([16, 3, 128, 128])
torch.Size([16, 3, 128, 128])
iteration step 6, G_loss: 0.11382, step_acc: 13.37391, grid_scale: 4 / 5, strokes: 4 / 4
iteration step 7, G_loss: 0.11198, step_acc: 13.44106, grid_scale: 4 / 5, strokes: 4 / 4
rendering canvas...
saving final rendered result...
READING batch_2_out.pkl
torch.Size([25, 3, 128, 128])
torch.Size([25, 3, 128, 128])
torch.Size([25, 3, 128, 128])
iteration step 0, G_loss: 0.13144, step_acc: 13.99752, grid_scale: 5 / 5, strokes: 1 / 4
iteration step 1, G_loss: 0.12878, step_acc: 14.16181, grid_scale: 5 / 5, strokes: 1 / 4
torch.Size([25, 3, 128, 128])
torch.Size([25, 3, 128, 128])
iteration step 2, G_loss: 0.12150, step_acc: 14.60731, grid_scale: 5 / 5, strokes: 2 / 4
iteration step 3, G_loss: 0.11936, step_acc: 14.76217, grid_scale: 5 / 5, strokes: 2 / 4
torch.Size([25, 3, 128, 128])
torch.Size([25, 3, 128, 128])
iteration step 4, G_loss: 0.11273, step_acc: 15.25421, grid_scale: 5 / 5, strokes: 3 / 4
iteration step 5, G_loss: 0.11055, step_acc: 15.39045, grid_scale: 5 / 5, strokes: 3 / 4
torch.Size([25, 3, 128, 128])
torch.Size([25, 3, 128, 128])
iteration step 6, G_loss: 0.10486, step_acc: 15.83623, grid_scale: 5 / 5, strokes: 4 / 4
iteration step 7, G_loss: 0.10189, step_acc: 16.04566, grid_scale: 5 / 5, strokes: 4 / 4
rendering canvas...
saving final rendered result...
READING batch_3_out.pkl
(800, 600, 3)


